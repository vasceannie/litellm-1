# model_list:
#   - model_name: "*"
#     litellm_params:
#       model: openai/*
#       api_key: os.environ/OPENAI_API_KEY
#   # provider specific wildcard routing
#   - model_name: "anthropic/*"
#     litellm_params:
#       model: "anthropic/*"
#       api_key: os.environ/ANTHROPIC_API_KEY
#   - model_name: "groq/*"
#     litellm_params:
#       model: "groq/*"
#       api_key: os.environ/GROQ_API_KEY
#   - model_name: "gemini/*"
#     litellm_params:
#       model: "gemini/*"
#       api_key: os.environ/GEMINI_API_KEY
#   - model_name: "deepseek/*"
#     litellm_params:
#       model: "deepseek/*"
#       api_key: os.environ/DEEPSEEK_API_KEY
#   - model_name: "groq/*"
#     litellm_params:
#       model: "groq/*"
#       api_key: os.environ/GROQ_API_KEY
#   - model_name: "cohere/*"
#     litellm_params:
#       model: "cohere/*"
#       api_key: os.environ/COHERE_API_KEY
litellm_settings:
  # set_verbose: True  # Uncomment this if you want to see verbose logs; not recommended in production
  drop_params: True
  # max_budget: 100 
  # budget_duration: 30d
  num_retries: 5
  request_timeout: 600
  telemetry: True
  context_window_fallbacks: [{"gpt-3.5-turbo": ["gpt-3.5-turbo-large"]}]
  default_team_settings: 
    - team_id: Tiger Team Alpha
      success_callback: ["langfuse"]
      failure_callback: ["langfuse"]
      langfuse_public_key: os.environ/LANGFUSE_PUBLIC_KEY
      langfuse_secret: os.environ/LANGFUSE_SECRET_KEY

# For /fine_tuning/jobs endpoints
# finetune_settings:
#   - custom_llm_provider: azure
#     api_base: os.environ/AZURE_API_BASE
#     api_key: os.environ/AZURE_API_KEY
#     api_version: "2023-03-15-preview"
#   - custom_llm_provider: openai
#     api_key: os.environ/OPENAI_API_KEY

# # for /files endpoints
# files_settings:
#   - custom_llm_provider: azure
#     api_base: os.environ/AZURE_API_BASE
#     api_key: os.environ/AZURE_API_KEY
#     api_version: "2023-03-15-preview"
#   - custom_llm_provider: openai
#     api_key: os.environ/OPENAI_API_KEY

router_settings:
  routing_strategy: usage-based-routing-v2 
  redis_host: os.environ/REDIS_HOST
  redis_password: os.environ/REDIS_PASSWORD
  redis_port: os.environ/REDIS_PORT
  redis_username: os.environ/REDIS_USER
  enable_pre_call_checks: true
  # model_group_alias: {"my-special-fake-model-alias-name": "fake-openai-endpoint-3"} 

general_settings: 
  master_key: sk-13a78f5beafdb9f46f28217f67ea45833ff03a058e87b8c2b972af0b102bdb9a
  litellm_salt_key: sk-13a78f5beafdb9f46f28217f67ea45833ff03a058e87b8c2b972af0b102bdb9a
  store_model_in_db: True
  proxy_budget_rescheduler_min_time: 60
  proxy_budget_rescheduler_max_time: 64
  proxy_batch_write_at: 1
  database_connection_pool_limit: 10
  database_url: "postgresql://vasceannie:%24QUirtle123@192.168.0.200:5432/litellm" # [OPTIONAL] use for token-based auth to proxy

  pass_through_endpoints:
    - path: "/v1/rerank"                                  # route you want to add to LiteLLM Proxy Server
      target: "https://api.cohere.com/v1/rerank"          # URL this route should forward requests to
      headers:                                            # headers to forward to this URL
        content-type: application/json                    # (Optional) Extra Headers to pass to this endpoint 
        accept: application/json
      forward_headers: True

# environment_variables:
#   # settings for using redis caching
#   REDIS_HOST: os.environ/REDIS_HOST
#   REDIS_PORT: os.environ/REDIS_PORT
#   REDIS_PASSWORD: os.environ/REDIS_PASSWORD
#   REDIS_USER: os.environ/REDIS_USER